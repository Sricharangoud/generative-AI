{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sricharangoud/generative-AI/blob/main/genai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        " values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        " with the outcomes of libraries\n",
        " YActual YP red\n",
        " 20 20.5\n",
        " 30 30.3\n",
        " 40 40.2\n",
        " 50 50.6\n",
        " 60 60.7\n",
        " Tabela 1: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "UAcihyTngFSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# Table 1 data\n",
        "Y_actual = [20, 30, 40, 50, 60]\n",
        "Y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n"
      ],
      "metadata": {
        "id": "NpV4q9_8gNqZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate MAE\n",
        "def calculate_mae(y_actual, y_pred):\n",
        "  absolute_errors = [abs(a - p) for a, p in zip(y_actual, y_pred)]\n",
        "  return sum(absolute_errors) / len(y_actual)"
      ],
      "metadata": {
        "id": "yUH-HeAYgeqz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate MSE\n",
        "def calculate_mse(y_actual, y_pred):\n",
        "    squared_errors = [(a - p) ** 2 for a, p in zip(y_actual, y_pred)]\n",
        "    return sum(squared_errors) / len(y_actual)"
      ],
      "metadata": {
        "id": "rF7ttt3XhFO7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate RMSE\n",
        "def calculate_rmse(y_actual, y_pred):\n",
        "    mse = calculate_mse(y_actual, y_pred)\n",
        "    return mse ** 0.5\n",
        "# Compute metrics from scratch\n",
        "mae_manual = calculate_mae(Y_actual, Y_pred)\n",
        "mse_manual = calculate_mse(Y_actual, Y_pred)\n",
        "rmse_manual = calculate_rmse(Y_actual, Y_pred)"
      ],
      "metadata": {
        "id": "keqpf0ZUhS95"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics using libraries\n",
        "mae_library = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_library = mean_squared_error(Y_actual, Y_pred)\n",
        "rmse_library = np.sqrt(mse_library)"
      ],
      "metadata": {
        "id": "mCTAECeLhYkm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_manual}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_manual}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_manual}\")\n",
        "print(\"\\nLibrary Calculations:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_library}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_library}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_library}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZh87Maihs7V",
        "outputId": "af12beec-deef-44d3-e04d-e4012b639d9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Library Calculations:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red"
      ],
      "metadata": {
        "id": "O8d23P6HiAYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Table 2 data\n",
        "Y_actual = [\n",
        "    [0, 0, 1],\n",
        "    [1, 2, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1],\n",
        "    [2, 2, 1],\n",
        "    [0, 2, 1]\n",
        "]\n",
        "Y_pred = [\n",
        "    [0, 0, 1],\n",
        "    [0, 2, 0],\n",
        "    [0, 1, 1],\n",
        "    [2, 0, 1],\n",
        "    [2, 2, 2],\n",
        "    [0, 2, 1]\n",
        "]"
      ],
      "metadata": {
        "id": "NAFGj_Fgiyue"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the lists for metric calculations\n",
        "y_actual_flat = [label for row in Y_actual for label in row]\n",
        "y_pred_flat = [label for row in Y_pred for label in row]\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(y_actual, y_pred):\n",
        "    correct_predictions = sum(1 for a, p in zip(y_actual, y_pred) if a == p)\n",
        "    return correct_predictions / len(y_actual)"
      ],
      "metadata": {
        "id": "qXmYcroai26u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate precision\n",
        "def calculate_precision(y_actual, y_pred):\n",
        "    true_positive = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)\n",
        "    predicted_positive = sum(1 for p in y_pred if p == 1)\n",
        "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n"
      ],
      "metadata": {
        "id": "4_XpLbvfi6Tm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate recall\n",
        "def calculate_recall(y_actual, y_pred):\n",
        "    true_positive = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)\n",
        "    actual_positive = sum(1 for a in y_actual if a == 1)\n",
        "    return true_positive / actual_positive if actual_positive > 0 else 0\n",
        "\n",
        "# Function to calculate F1 Score\n",
        "def calculate_f1(precision, recall):\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n"
      ],
      "metadata": {
        "id": "YAIA33chi-X4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics from scratch\n",
        "accuracy_manual = calculate_accuracy(y_actual_flat, y_pred_flat)\n",
        "precision_manual = calculate_precision(y_actual_flat, y_pred_flat)\n",
        "recall_manual = calculate_recall(y_actual_flat, y_pred_flat)\n",
        "f1_manual = calculate_f1(precision_manual, recall_manual)\n",
        "\n",
        "# Compute metrics using libraries\n",
        "accuracy_library = accuracy_score(y_actual_flat, y_pred_flat)\n",
        "precision_library = precision_score(y_actual_flat, y_pred_flat, average='macro')\n",
        "recall_library = recall_score(y_actual_flat, y_pred_flat, average='macro')\n",
        "f1_library = f1_score(y_actual_flat, y_pred_flat, average='macro')\n"
      ],
      "metadata": {
        "id": "7WyR3E7YjBlH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_manual}\")\n",
        "print(f\"Precision: {precision_manual}\")\n",
        "print(f\"Recall: {recall_manual}\")\n",
        "print(f\"F1 Score: {f1_manual}\")\n",
        "\n",
        "print(\"\\nLibrary Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_library}\")\n",
        "print(f\"Precision: {precision_library}\")\n",
        "print(f\"Recall: {recall_library}\")\n",
        "print(f\"F1 Score: {f1_library}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o07I9inFjEyS",
        "outputId": "e7928152-8053-41c8-901a-8093db35ef49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Accuracy: 0.7222222222222222\n",
            "Precision: 0.8\n",
            "Recall: 0.5714285714285714\n",
            "F1 Score: 0.6666666666666666\n",
            "\n",
            "Library Calculations:\n",
            "Accuracy: 0.7222222222222222\n",
            "Precision: 0.726984126984127\n",
            "Recall: 0.7619047619047619\n",
            "F1 Score: 0.7269841269841271\n"
          ]
        }
      ]
    }
  ]
}